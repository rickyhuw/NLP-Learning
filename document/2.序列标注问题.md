## 2.序列标注问题

### 1.现状
​	序列标注问题是自然语言中最常见的问题，在深度学习火起来之前，常见的序列标注问题的解决方案都是借助于HMM模型，最大熵模型，CRF模型。尤其是CRF，是解决序列标注问题的主流方法。随着深度学习的发展，RNN在序列标注问题中取得了巨大的成果。而且深度学习中的end-to-end，也让序列标注问题变得更简单了。
​	序列标注问题包括自然语言处理中的**分词，词性标注，命名实体识别，关键词抽取，词义角色标注**等等。我们只要在做序列标注时给定特定的标签集合，就可以进行序列标注。
​	序列标注问题是NLP中最常见的问题，因为绝大多数NLP问题都可以转化为序列标注问题，虽然很多NLP任务看上去大不相同，但是如果转化为序列标注问题后其实面临的都是同一个问题。所谓“序列标注”，就是说对于一个一维线性输入序列：
$$
X = [x_1,x_2,x_3…]
$$
给线性序列中的每个元素打上标签集合中的某个标签：
$$
Y=[y_1,y_2,y_3,...]
$$
​	所以，其本质上是对线性序列中每个元素根据上下文内容进行分类的问题。一般情况下，对于NLP任务来说，线性序列就是输入的文本，往往可以把一个汉字看做线性序列的一个元素，而不同任务其标签集合代表的含义可能不太相同，但是相同的问题都是：如何根据汉字的上下文给汉字打上一个合适的标签（无论是分词，还是词性标注，或者是命名实体识别，道理都是想通的）。

### 2.分词

目前，市面上的分词工具有很多，比如jieba,pkusge,hanlp等，在实际应用中简单的分词任务也可以直接拿过来使用，但是背后的算法思想还是有必要知道的。（虽然不知道这些工具背后的真正的算法，但是可以去揣测一番）

* 基于词典的方式，比较传统的方式了，需要提前准备好人工标注好的词典库。
  * 前向最大匹配
  * 后向最大匹配
* 基于模型的方式，这种方式比较符合当下的序列问题。
  * 首先需要有标注数据，可以将每个字标注为：label = {B,M,E,S}，分别表示{begin，medio，end，single}
  * 对句子进行拆词，把句子拆分为单个的字
  * 根据标签计算切词，并计算概率。
  * 切词
* 基于模型的方式一般是使用双向LSTM，保证训练过程关注句子的前后信息。
* 在NLP中最直观的处理问题的方式就是要把问题转换为序列标注问题，思考问题的思维方式也就转换为序列标注思维，这个思维很重要，决定你能否真的处理好NLP问题。

### 3. 命名实体识别（NER）

命名实体识别任务的处理，只要理解了上面的分词处理过程，大致的思想是一样的，标注和序列分类是两个关键步骤，标注一般都是**人工标注+正则标注**（一般是BIOS方式），序列分类就是应用深度网络算法进行学习特征，通过CRF来保证文字的序列性（转移矩阵）

除了上面的分词和命名实体标注，很多其他的NLP问题同样可以转换为序列标注问题，比如词性标注、CHUNK识别、句法分析、语义角色识别、关键词抽取等。

传统解决序列标注问题的方法包括HMM/MaxEnt/CRF等，很明显RNN很快会取代CRF的主流地位，成为解决序列标注问题的标准解决方案，那么如果使用RNN来解决各种NLP基础及应用问题，我们又该如何处理呢，下面我们就归纳一下使用RNN解决序列标注问题的一般优化思路。

对于分词、词性标注（POS）、命名实体识别（NER）这种前后依赖不会太远的问题，可以用RNN或者BiRNN处理就可以了。而对于具有长依赖的问题，可以使用LSTM、RLSTM、GRU等来处理。关于GRU和LSTM两者的性能差不多，不过对于样本数量较少时，有限考虑使用GRU（模型结构较LSTM更简单）。此外神经网络在训练的过程中容易过拟合，可以在训练过程中加入Dropout或者L1/L2正则来避免过拟合。

### 4.常用算法优劣

* **LSTM:**像RNN、LSTM、BILSTM这些模型，它们在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方，对于t时刻来说，输出层`y_t`受到隐层`h_t`（包含上下文信息）和输入层`x_t`（当前的输入）的影响，但是y_t和其他时刻的`y_t`是相互独立的，感觉像是一种point wise，对当前t时刻来说，我们希望找到一个概率最大的`y_t`，但其他时刻的`y_t`对当前`y_t`没有影响，如果yt之间存在较强的依赖关系的话（例如，形容词后面一般接名词，存在一定的约束），LSTM无法对这些约束进行建模，LSTM模型的性能将受到限制。(简单来讲，就是LSTM缺少了对序列的约束。)
* **CRF:**它不像LSTM等模型，能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的yt，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种联合概率，优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。
* **HMM:**CRF不管是在实践还是理论上都要优于HMM，HMM模型的参数主要是“初始的状态分布”，“状态之间的概率转移矩阵”，“状态到观测的概率转移矩阵”，这些信息在CRF中都可以有，例如：在特征模版中考虑h(y1), f(yi-1, yi), g(yi, xi)等特征。
* **CRF与LSTM：**从数据规模来说，在数据规模较小时，CRF的试验效果要略优于BILSTM，当数据规模较大时，BILSTM的效果应该会超过CRF。从场景来说，如果需要识别的任务不需要太依赖长久的信息，此时RNN等模型只会增加额外的复杂度，此时可以考虑类似科大讯飞**FSMN**（一种基于窗口考虑上下文信息的“前馈”网络）。
* **CNN＋BILSTM＋CRF：**这是目前学术界比较流行的做法，BILSTM＋CRF是为了结合以上两个模型的优点，CNN主要是处理英文的情况，英文单词是由更细粒度的字母组成，这些字母潜藏着一些特征（例如：前缀后缀特征），通过CNN的卷积操作提取这些特征，在中文中可能并不适用（中文单字无法分解，除非是基于分词后），这里简单举一个例子，例如词性标注场景，单词football与basketball被标为名词的概率较高， 这里后缀ball就是类似这种特征。

### 5.Reference

* [CRF和LSTM 模型在序列标注上的优劣？](https://www.zhihu.com/question/46688107/answer/117448674)
* [使用RNN解决NLP中序列标注问题的通用优化思路](https://blog.csdn.net/malefactor/article/details/50725480)
* [使用深度学习进行中文自然语言处理之序列标注](https://www.jianshu.com/p/7e233ef57cb6)